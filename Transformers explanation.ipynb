{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPdxZM7VEsi3OoYacFtN/A+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# What are transformers\n","Transformers are language models. \n","\n","All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as language models. This means they have been trained on large amounts of raw text in a self-supervised fashion. \n","\n","# Self-supervised learning \n","Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!\n","\n","This type of model develops a **statistical** understanding of the language it has been trained on, but it’s not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called transfer learning. During this process, the model is fine-tuned in a supervised way — that is, using **human-annotated labels** — on a given task.\n","\n","- causal language modeling\n","- masked language modeling\n","\n","causal language modeling: the output depends on the past and present inputs, but not the future ones.\n","\n","masked language modeling: the model predicts a masked word in the sentence.\n","\n","\n","# Fine-tuning\n","Fine-tuning \n","is the training done after a model has been pretrained. To perform fine-tuning, you first **acquire a pretrained language model**, then perform **additional training** with a dataset specific to your task. \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"-Viq57vxxtfl"}},{"cell_type":"markdown","source":["# pipline \n","\n","```\n","classifier = pipeline(\"sentiment-analysis\")\n","```\n","\n","`pipeline()` groups together three steps: \n","- preprocessing, (raw text --> IDs)\n","- passing the inputs through the model, (IDs --> logits)\n","- postprocessing(logits --> predictions/possibilities)\n","\n","\n","# Tokenizer\n","```\n","from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","```\n","Using the checkpoint name of our model, it will automatically fetch the data associated with the model’s tokenizer and cache it (so it’s only **downloaded** the first time you run the code below).\n","\n","\n","```\n","raw_inputs = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"I hate this so much!\",\n","]\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","print(inputs)\n","```\n","\n","A `tokenizer` is responsible for:\n","- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n","- Mapping each token to an `integer`\n","- Adding `additional inputs ` that may be useful to the model\n","\n","All this `preprocessing` needs to be done in exactly the same way as when the `model` was pretrained\n","\n","\n","# `AutoTokenizer` class and its `from_pretrained()` method.\n","\n","```\n","from transformers import AutoTokenizer\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","```\n","Once we have the tokenizer, we can directly pass our sentences to it and we’ll get back a `dictionary` that’s ready to feed to our model! \n","\n","The only thing left to do is to convert the `list of input IDs `to `tensors`.\n","\n"," Transformer models only accept **`tensors` as input**. \n","If this is your first time hearing about tensors, you can think of them as **NumPy arrays** instead. \n","\n","## `return_tensors `\n","To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the `return_tensors `argument:\n","\n","```\n","raw_inputs = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"I hate this so much!\",\n","]\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","print(inputs)\n","```\n","\n","Here’s what the results look like as PyTorch tensors:\n","\n","```\n","{\n","    'input_ids': tensor([\n","        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],\n","        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]\n","    ]), \n","    'attention_mask': tensor([\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n","    ])\n","}\n","```\n","The output itself is a dictionary containing two keys, `input_ids` and `attention_mask`. \n","\n","# the model\n","\n","We can download our pretrained model the same way we did with our tokenizer.\n","\n","\n","\n","```\n","from transformers import AutoModel\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","\n","model = AutoModel.from_pretrained(checkpoint)\n","```\n","\n","In this code snippet, we have **downloaded the same checkpoint** we used in our pipeline before (it should actually have been cached already) and **instantiated a model** with it.\n","\n","\n","This architecture is responsible for: given some inputs, it outputs what we’ll call **hidden states**, also known as features. \n","\n","For each model **input**, we’ll retrieve a **high-dimensional vector** representing the contextual understanding of that input by the Transformer model.\n","\n","# A high-dimensional vector\n","\n","The vector output by the Transformer module is usually large. It generally has three dimensions:\n","\n","- Batch size: The number of sequences processed at a time (2 in our example).\n","\n","- equence length: The length of the numerical representation of the sequence (16 in our example).\n","- Hidden size: The vector dimension of each model input.\n","\n","### head \n","\n","While these hidden states can be useful on their own, they’re usually inputs to another part of the model, known as the **head**.\n","\n","If you are performing the task the model was pre-trained on, you can simply model the input:\n","\n","```\n","outputs = model(**inputs)\n","```\n","\n","```\n","print(outputs.last_hidden_state.shape)\n","```\n","\n","The result:\n","```\n","torch.Size([2, 16, 768])\n","\n","```\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"JKrL6prQLH1g"}},{"cell_type":"code","source":[],"metadata":{"id":"JAZcKc2FAySk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iuRPjzdf9zQo"},"execution_count":null,"outputs":[]}]}